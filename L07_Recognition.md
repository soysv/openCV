## 1ï¸âƒ£MNIST ì†ê¸€ì”¨ ë¶„ë¥˜ë¥¼ ìœ„í•œ ì‹ ê²½ë§ ëª¨ë¸
### ğŸŒ€ ê³¼ì œ ì„¤ëª…
- MNIST ì†ê¸€ì”¨ ì´ë¯¸ì§€(0~9 ìˆ«ì)ë¥¼ ë¶„ë¥˜í•˜ëŠ” ê°„ë‹¨í•œ ì‹ ê²½ë§(MLP) ëª¨ë¸ êµ¬í˜„
- ë°ì´í„° ì „ì²˜ë¦¬, ëª¨ë¸ êµ¬ì„±, í•™ìŠµ ë° í‰ê°€ë¥¼ í¬í•¨
<br>

  
### ğŸ“Œ ê°œë…
- <b>Flatten</b> <br>
<p> : 2D ì´ë¯¸ì§€ë¥¼ 1D ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ Dense ì¸µì— ì „ë‹¬

- <b>One-Hot Encoding</b> <br>
<p> : ì •ìˆ˜ ë ˆì´ë¸”ì„ ì´ì§„ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ë¶„ë¥˜ì— ì í•©í•œ í˜•íƒœë¡œ ë³€ê²½

- <b>Softmax</b>: í´ë˜ìŠ¤ë³„ í™•ë¥ ì„ ì¶œë ¥í•˜ì—¬ ê°€ì¥ ë†’ì€ ê°’ì„ ê°€ì§„ í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡
<br>

### ğŸ’» ì£¼ìš” ì½”ë“œ
<p>âœ” <b> 1. ë°ì´í„° ë¡œë“œ ë° ì •ê·œí™” </b><br><p><code>(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train = x_train / 255.0
x_test = x_test / 255.0
</code></p>
<p>  -load_data(): MNIST í›ˆë ¨/í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°<br>
<br>
<p>âœ” <b> 2. ë¼ë²¨ ì¸ì½”ë”© (One-Hot)</b><br> <p><code>from tensorflow.keras.utils import to_categorical
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)
</code><br>
<p>  - ì •ìˆ˜í˜• í´ë˜ìŠ¤ ë ˆì´ë¸”ì„ 10ì°¨ì› ì´ì§„ ë²¡í„°ë¡œ ë³€í™˜
<br>
<p>âœ” <b> 3. ì‹ ê²½ë§ ëª¨ë¸ êµ¬ì„±</b><br> 
<p><code>model = Sequential([
    Flatten(input_shape=(28, 28)),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')
])
</code>
<p> - Flatten: 28x28 ì´ë¯¸ì§€ë¥¼ 784ì°¨ì› ë²¡í„°ë¡œ ë°”ê¿ˆ<br>
<p> - Dense(128): ì€ë‹‰ì¸µ (ReLU í™œì„±í™”)<br>
<p> - Dense(10, softmax): ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ìš© ì¶œë ¥ì¸µ<br>
<br>
<p>âœ” <b> 4. ëª¨ë¸ ì»´íŒŒì¼</b><br> 
<p><code>model = Sequential([
    Flatten(input_shape=(28, 28)),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')
])
</code><br>
<p> - Flatten: 28x28 ì´ë¯¸ì§€ë¥¼ 784ì°¨ì› ë²¡í„°ë¡œ ë°”ê¿ˆ<br>
<p> - Dense(128): ì€ë‹‰ì¸µ (ReLU í™œì„±í™”)<br>
<p> - Dense(10, softmax): ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ìš© ì¶œë ¥ì¸µ<br>

<br>
<br>



<details>
  <summary><b> ğŸ§¿ í´ë¦­í•´ì„œ ì½”ë“œ ë³´ê¸° </b></summary>
  
  ```python
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.utils import to_categorical

# 1. MNIST ë°ì´í„°ì…‹ ë¡œë“œ
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# 2. ë°ì´í„° ì „ì²˜ë¦¬
# í”½ì…€ ê°’ ì •ê·œí™” (0~255 â†’ 0~1)
x_train = x_train / 255.0
x_test = x_test / 255.0

# ë¼ë²¨ì„ one-hot encoding
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

# 3. ê°„ë‹¨í•œ ì‹ ê²½ë§ ëª¨ë¸ êµ¬ì„±
model = Sequential([
    Flatten(input_shape=(28, 28)),  # 28x28 ì´ë¯¸ì§€ë¥¼ 1ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜
    Dense(128, activation='relu'), # ì€ë‹‰ì¸µ (128ê°œì˜ ë‰´ëŸ°, ReLU í™œì„±í™” í•¨ìˆ˜)
    Dense(10, activation='softmax') # ì¶œë ¥ì¸µ (10ê°œì˜ ìˆ«ì ë¶„ë¥˜)
])

# 4. ëª¨ë¸ ì»´íŒŒì¼
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# 5. ëª¨ë¸ í›ˆë ¨
model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.1)

# 6. ëª¨ë¸ í‰ê°€
test_loss, test_accuracy = model.evaluate(x_test, y_test)
print(f"\nâœ… í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_accuracy:.4f}")


 ```
</details>

<br>

### ğŸ•µâ€â™€ ê²°ê³¼í™”ë©´
![ê²°ê³¼ì´ë¯¸ì§€](./data/6_1.png)



### ğŸ“Œ ì› í¬ê¸°ê°€ ë‹¤ë¥¸ ì´ìœ 
- ì›ì˜ í¬ê¸°ëŠ” í•´ë‹¹ íŠ¹ì§•ì ì´ ê²€ì¶œëœ <b>ìŠ¤ì¼€ì¼(í¬ê¸°)</b>ì„ ë‚˜íƒ€ëƒ„.
- í° ì›ì¼ìˆ˜ë¡ ë” í° ì˜ì—­ì„ ëŒ€í‘œí•˜ëŠ” íŠ¹ì§•ì .
- ì‘ì€ ì›ì¼ìˆ˜ë¡ ë” ì‘ì€ ì˜ì—­ì„ ëŒ€í‘œí•˜ëŠ” íŠ¹ì§•ì .

<br>
<br>

## 2ï¸âƒ£ CIFAR-10ì„ í™œìš©í•œ CNN ì´ë¯¸ì§€ ë¶„ë¥˜ê¸°
### ğŸŒ€ ê³¼ì œ ì„¤ëª…
- CIFAR-10 ì´ë¯¸ì§€ ë°ì´í„°ì…‹ì„ ì´ìš©í•˜ì—¬ í•©ì„±ê³± ì‹ ê²½ë§(CNN)ì„ ì„¤ê³„í•˜ê³  í•™ìŠµ
- í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ì— ëŒ€í•´ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì‹œê°í™”
<br>

### ğŸ“Œ ê°œë…
- <b>CNN (Convolutional Neural Network)</b><br>
<p> : ì´ë¯¸ì§€ ë¶„ë¥˜ì— ìì£¼ ì“°ì´ëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸. Conv2D, MaxPooling, Flatten, Dense ê³„ì¸µìœ¼ë¡œ êµ¬ì„±ë¨
- <b>ì •ê·œí™” (Normalization)</b><br>
<p> : ì…ë ¥ ì´ë¯¸ì§€ì˜ í”½ì…€ ê°’ì„ 0~1 ë²”ìœ„ë¡œ ìŠ¤ì¼€ì¼ë§í•˜ì—¬ í•™ìŠµ ì•ˆì •ì„±ê³¼ ì†ë„ í–¥ìƒ
<br>
  <br>
<br>

### ğŸ’» ì£¼ìš” ì½”ë“œ
<p>âœ” <b>CIFAR-10 ë°ì´í„° ë¡œë“œ ë° í´ë˜ìŠ¤ ì •ì˜</b><br> <p><code>from tensorflow.keras.datasets import cifar10
(x_train, y_train), (x_test, y_test) = cifar10.load_data()
class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck']
</code><br>
<p>  - load_data(): í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¶„ë¦¬<br>
<p>  - class_names: ì •ìˆ˜ ë ˆì´ë¸”ì„ ë¬¸ìì—´ë¡œ ë§¤í•‘
  
<p>âœ” <b>ë°ì´í„° ì „ì²˜ë¦¬ (ì •ê·œí™”)
</b><br> <p><code>x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0
</code><br>
<p>- ì •ê·œí™”ë¥¼ í†µí•´ ê°’ì˜ ë²”ìœ„ë¥¼ 0~1ë¡œ ì¡°ì • â†’ ë¹ ë¥´ê³  ì•ˆì •ì ì¸ í•™ìŠµ ìœ ë„<br>
  
<p>âœ” <b>CNN ëª¨ë¸ êµ¬ì„±</b><br> <p><code>model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(64, (3, 3), activation='relu'),

    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(10, activation='softmax')  # CIFAR-10ì€ 10ê°œ í´ë˜ìŠ¤
])
</code><br>
<p> - Conv2D: ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œì„ ìœ„í•œ í•„í„° ì ìš©
<p> - MaxPooling2D: íŠ¹ì§•ë§µì˜ í¬ê¸° ê°ì†Œ â†’ ì—°ì‚°ëŸ‰ ì¤„ì´ê¸°
<p> - Flatten: 2D â†’ 1D ë²¡í„°ë¡œ ë³€í™˜
<p> - Dense: ì¶œë ¥ì¸µ í¬í•¨í•œ Fully Connected Layer
<p> - Softmax: ë‹¤ì¤‘ í´ë˜ìŠ¤ í™•ë¥  ì¶œë ¥
<br>
  
<p>âœ” <b>ëª¨ë¸ ì»´íŒŒì¼ ë° í•™ìŠµ</b><br> <p><code>model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

history = model.fit(x_train, y_train, epochs=10,
                    validation_data=(x_test, y_test))

</code><br>
<p> - Adam: í•™ìŠµë¥  ì¡°ì ˆì— ë›°ì–´ë‚œ ì˜µí‹°ë§ˆì´ì €<br>
<p> - sparse_categorical_crossentropy: ì •ìˆ˜ ì¸ì½”ë”©ëœ í´ë˜ìŠ¤ ë ˆì´ë¸”ì— ì í•©í•œ ì†ì‹¤ í•¨ìˆ˜
<br>
<br>

<details>
  <summary><b> ğŸ§¿ í´ë¦­í•´ì„œ ì½”ë“œ ë³´ê¸° </b></summary>

  ```python
import tensorflow as tf
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt
import numpy as np

# 1. CIFAR-10 ë°ì´í„°ì…‹ ë¡œë“œ
from tensorflow.keras.datasets import cifar10

(x_train, y_train), (x_test, y_test) = cifar10.load_data()

# í´ë˜ìŠ¤ ì´ë¦„ ì •ì˜
class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck']

print("Train shape:", x_train.shape)
print("Test shape:", x_test.shape)

# 2. ë°ì´í„° ì „ì²˜ë¦¬: í”½ì…€ ì •ê·œí™” (0~255 â†’ 0~1)
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

# 3. CNN ëª¨ë¸ êµ¬ì„±
model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(64, (3, 3), activation='relu'),

    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(10, activation='softmax')  # CIFAR-10ì€ 10ê°œ í´ë˜ìŠ¤
])

# 4. ëª¨ë¸ ì»´íŒŒì¼
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# 5. ëª¨ë¸ í›ˆë ¨
history = model.fit(x_train, y_train, epochs=10,
                    validation_data=(x_test, y_test))

# 6. ì„±ëŠ¥ í‰ê°€
test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)
print(f"\nâœ… í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc:.4f}")

# 7. ì˜ˆì¸¡ ìˆ˜í–‰ (í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ì¼ë¶€ ì‹œê°í™”)
predictions = model.predict(x_test)

# 8. ê²°ê³¼ ì‹œê°í™” í•¨ìˆ˜
def plot_image(i, predictions_array, true_label, img):
    true_label, img = true_label[i][0], img[i]
    plt.grid(False)
    plt.xticks([])
    plt.yticks([])

    plt.imshow(img)

    predicted_label = np.argmax(predictions_array)
    color = 'blue' if predicted_label == true_label else 'red'

    plt.xlabel(f"Prediction: {class_names[predicted_label]}\nLabel: {class_names[true_label]}", color=color)

# 9. ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥ (5ê°œ ì´ë¯¸ì§€)
plt.figure(figsize=(10, 5))
for i in range(5):
    plt.subplot(1, 5, i + 1)
    plot_image(i, predictions[i], y_test, x_test)
plt.tight_layout()
plt.show()


 ```
</details>

<br>

### ğŸ•µâ€â™€ ê²°ê³¼í™”ë©´
![ê²°ê³¼ì´ë¯¸ì§€](./data/6_2.png)

<br>
<br>

## 3ï¸âƒ£ VGG16ì„ í™œìš©í•œ ì „ì´ í•™ìŠµ ê¸°ë°˜ ì´ë¯¸ì§€ ë¶„ë¥˜ê¸°
### ğŸŒ€ ê³¼ì œ ì„¤ëª…
- ì‚¬ì „ í•™ìŠµëœ VGG16 ëª¨ë¸ì„ í™œìš©í•˜ì—¬ CIFAR-10 ë°ì´í„°ì…‹ ë¶„ë¥˜ê¸° ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚´
- VGG16ì˜ Feature Extractorë¡œì„œì˜ ì„±ëŠ¥ì„ ì´ìš©í•˜ê³ , ê·¸ ìœ„ì— ìƒˆë¡œìš´ ë¶„ë¥˜ê¸°ë¥¼ ìŒ“ì•„ í•™ìŠµ
<br>

### ğŸ“Œ ê°œë…
- <b>ì „ì´ í•™ìŠµ (Transfer Learning)</b>
<p>: ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì—ì„œ í•™ìŠµëœ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ê°€ì ¸ì™€ ìƒˆë¡œìš´ ê³¼ì œì— ì¬í™œìš©í•˜ëŠ” ê¸°ë²•. ì ì€ ë°ì´í„°ë¡œë„ ë†’ì€ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆìŒ.</p>

- <b>VGG16</b>
<p>: ImageNet ë°ì´í„°ì…‹ì— ëŒ€í•´ í•™ìŠµëœ ê¹Šì€ CNN ëª¨ë¸. `tensorflow.keras.applications`ì—ì„œ ì œê³µë¨.</p> <p>: `include_top=False`ë¡œ ì„¤ì •í•˜ë©´, ìµœì¢… Fully Connected Layerë¥¼ ì œê±°í•œ **íŠ¹ì§• ì¶”ì¶œê¸°**ë¡œ í™œìš© ê°€ëŠ¥</p>

- <b>CIFAR-10</b>

<p>: ì´ 10ê°œì˜ ì´ë¯¸ì§€ í´ë˜ìŠ¤ë¥¼ ê°€ì§„ ì†Œí˜• ì»¬ëŸ¬ ì´ë¯¸ì§€ ë°ì´í„°ì…‹ (32Ã—32 í¬ê¸°)</p> <p>: VGG16ì˜ ì…ë ¥ ìš”êµ¬(224Ã—224)ë¡œ í¬ê¸° ì¡°ì • í•„ìš”</p> <br>

### ğŸ’» ì£¼ìš” ì½”ë“œ
<p> âœ” <b> CIFAR-10 ë°ì´í„° ë¡œë“œ ë° ì •ê·œí™”</b> <br>
<p><code>(x_train, y_train), (x_test, y_test) = cifar10.load_data()
x_train = tf.image.resize(x_train, [224, 224]) / 255.0
x_test = tf.image.resize(x_test, [224, 224]) / 255.0</code><br>
<p> - resize(): CIFAR-10 ì´ë¯¸ì§€ë¥¼ VGG16ì´ ìš”êµ¬í•˜ëŠ” í¬ê¸°(224x224)ë¡œ ë³€ê²½
<p> - ì •ê·œí™”: ëª¨ë¸ í•™ìŠµ ì†ë„ í–¥ìƒì„ ìœ„í•´ 0~1 ë²”ìœ„ë¡œ ì¡°ì •
<br>

<p> âœ” <b> VGG16 ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ë° ê³ ì •</b><br>
 <p><code>base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
base_model.trainable = False</code><br>
<p> - include_top=False: FC Layer ì œê±° â†’ Feature Extractorë¡œ ì‚¬ìš©
<p> - trainable=False: ê¸°ì¡´ ê°€ì¤‘ì¹˜ë¥¼ ë™ê²° â†’ í•™ìŠµ ì‹œ ì—…ë°ì´íŠ¸ë˜ì§€ ì•ŠìŒ

<p> âœ” <b> ìƒˆë¡œìš´ ë¶„ë¥˜ê¸° ìŒ“ê¸° </b> <br>
<p><code>model = models.Sequential([
    base_model,
    layers.Flatten(),
    layers.Dense(256, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(10, activation='softmax')
])
</code>
<p> - Flatten(): Feature mapì„ 1D ë²¡í„°ë¡œ ë³€í™˜
<p> - Dense(256): ìƒˆë¡œìš´ Fully Connected Layer
<p> - Dropout: ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•´ 50% ë…¸ë“œ ë¹„í™œì„±í™”
<p> - Dense(10): CIFAR-10 í´ë˜ìŠ¤ ìˆ˜ì— ë§ì¶˜ ì¶œë ¥ì¸µ (Softmax)

<p> âœ”ï¸ <b> ëª¨ë¸ ì»´íŒŒì¼ ë° í•™ìŠµ</b><br>
<p><code>model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

history = model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))
</code><br>
<p> - optimizer='adam': ë¹ ë¥¸ ìˆ˜ë ´ì„ ìœ„í•œ ì˜µí‹°ë§ˆì´ì €
<p> - sparse_categorical_crossentropy: ì •ìˆ˜ í˜•íƒœì˜ ë ˆì´ë¸”ìš© ì†ì‹¤í•¨ìˆ˜
<p> - validation_data: ê²€ì¦ ì •í™•ë„ë¥¼ í•¨ê»˜ í™•ì¸í•˜ë©° í›ˆë ¨ ê°€ëŠ¥

<p> âœ”ï¸ <b> ì„±ëŠ¥ í‰ê°€</b><br>
<p><code>test_loss, test_acc = model.evaluate(x_test, y_test)
print(f"\nâœ… ì „ì´ í•™ìŠµ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc:.4f}")
</code><br>
<p> - í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ìµœì¢… ëª¨ë¸ í‰ê°€
<p> - evaluate(): ì†ì‹¤ê°’ê³¼ ì •í™•ë„ ì¶œë ¥
<br>
<br>

<br>
<details>
  <summary><b> ğŸ§¿ í´ë¦­í•´ì„œ ì½”ë“œ ë³´ê¸° </b></summary>

  ```python
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.applications import VGG16
from tensorflow.keras.datasets import cifar10
import numpy as np
import matplotlib.pyplot as plt

# 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
(x_train, y_train), (x_test, y_test) = cifar10.load_data()

# CIFAR-10ì€ (32,32,3)ì´ë¯€ë¡œ VGG16 (224,224,3)ë¡œ resize
x_train = tf.image.resize(x_train, [224, 224]) / 255.0
x_test = tf.image.resize(x_test, [224, 224]) / 255.0

# 2. VGG16 ë¶ˆëŸ¬ì˜¤ê¸° (ìµœìƒìœ„ ë ˆì´ì–´ ì œì™¸)
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# ì‚¬ì „í•™ìŠµëœ ê°€ì¤‘ì¹˜ ê³ ì • (Feature Extractorë¡œ ì‚¬ìš©)
base_model.trainable = False

# 3. ìƒˆ ë¶„ë¥˜ê¸° êµ¬ì„± (Fine-tuningìš©)
model = models.Sequential([
    base_model,
    layers.Flatten(),
    layers.Dense(256, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(10, activation='softmax')  # CIFAR-10 í´ë˜ìŠ¤ ìˆ˜
])

# 4. ëª¨ë¸ ì»´íŒŒì¼
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# 5. ëª¨ë¸ í•™ìŠµ
history = model.fit(x_train, y_train, epochs=10,
                    validation_data=(x_test, y_test))

# 6. ì„±ëŠ¥ í‰ê°€
test_loss, test_acc = model.evaluate(x_test, y_test)
print(f"\nâœ… ì „ì´ í•™ìŠµ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc:.4f}")
 ```
</details>

<br>

### ğŸ•µâ€â™€ ê²°ê³¼í™”ë©´
![ê²°ê³¼ì´ë¯¸ì§€](./data/6_3.png)
